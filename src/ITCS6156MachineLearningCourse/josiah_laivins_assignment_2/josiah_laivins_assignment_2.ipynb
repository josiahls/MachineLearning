{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2 - Classification\n",
    "\n",
    "<font color=\"red\"> <b> Due: Oct 11 (Thursday) 11:00 pm </b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> Josiah Laivins </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction\n",
    "\n",
    "Determining the qualities that play into what someone would call \"having satisfaction in their career\" is important. Through the use of multiple qualities based on the stack overflow 2018 survey, I hope to gain insight into what actually determines a satisfactory career."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data\n",
    "\n",
    "The link this data set can be found [this Kaggle Stack Overflow link](https://www.kaggle.com/stackoverflow/stack-overflow-2018-developer-survey)\n",
    "\n",
    "The data set found from stack overflow seemed interesting to use for classification. There should be interesting insights that I would expect to see. \n",
    "There are 98855 records found in this data set. There are 129 features, but I am going to list some main ones that I was immediately interested in:\n",
    "- SkipMeals\n",
    "- WakeTime\n",
    "- HoursComputer\n",
    "- RaceEthnicity\n",
    "- CareerSatisfaction\n",
    "\n",
    "As a note, I am going to binarize the target (CareerSatisfaction) via this method:\n",
    "- Extremely satisfied: 1\n",
    "- Moderately satisfied: 1\n",
    "- Slightly satisfied: 1\n",
    "- Neither satisfied nor dissatisfied': -1\n",
    "- Slightly dissatisfied: -1\n",
    "- Moderately dissatisfied: -1 \n",
    "- Extremely dissatisfied: -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Method\n",
    "\n",
    "Summarize the pocket algorithm, discriminant analysis, and logistic regression.\n",
    "The superclass *Classifier* defines common utility methods. \n",
    "Finish the normalize function for you. \n",
    "Do not forget explain your implementation. \n",
    "\n",
    "The explanation of your codes should not be the comments in a code cell. \n",
    "This section should include\n",
    " - review of the 4 classification models \n",
    " - your implementation and description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Super Classs Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def show_correlation(columns_to_look_at: List[str], data: pd.DataFrame, n_rows=200, is_labeled=False):\n",
    "    if not is_labeled:\n",
    "        data = data.reset_index(drop=True)\n",
    "        # Convert String columns into one-hot encoded columns\n",
    "        for column in data:\n",
    "            if data[column].dtype == object:\n",
    "                # print(f'Encoding one-hot for column: {column} \\r')\n",
    "                data[column] = LabelEncoder().fit_transform(y=data[column].fillna('0'))\n",
    "    else:\n",
    "        warnings.warn(\"Assuming the dataset is already labeled. Note, if it has strings, then this will be blank.\",\n",
    "                      category=RuntimeWarning)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.matshow(data[columns_to_look_at].corr())\n",
    "    plt.xticks(range(len(columns_to_look_at)), [column for column in data[columns_to_look_at].columns], rotation=45)\n",
    "    plt.yticks(range(len(columns_to_look_at)), [column for column in data[columns_to_look_at].columns], rotation=45)\n",
    "    plt.title(f'Prediction using: {n_rows} samples', y=1.15)\n",
    "    plt.ylabel('Columns Y')\n",
    "    plt.xlabel('Columns X')\n",
    "    plt.margins(.1)\n",
    "    plt.title(f'Prediction using: {n_rows}. Shows the correlation between different columns')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_accuracy(x, t):\n",
    "    # x, t = x[np.argsort(t, axis=0).flatten()], np.sort(t, axis=0)\n",
    "\n",
    "    plt.plot(t, label=\"Ground Truth\")\n",
    "    plt.plot(x, label=\"Result\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_boundaries(bounary_diff: np.array):\n",
    "    xs, ys = np.meshgrid(np.linspace(-3, 6, 500), np.linspace(-3, 7, 500))\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.contourf(xs, ys, (bounary_diff > 0).reshape(xs.shape))\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load data:\n",
    "base_dir = str(Path().absolute())\n",
    "n_rows = 20  # None for all\n",
    "data = pd.read_csv(base_dir + os.sep + 'data' + os.sep + 'stack-overflow-2018-developer-survey' + os.sep +\n",
    "                   'survey_results_public.csv', nrows=n_rows)\n",
    "\n",
    "# Features of interest:\n",
    "features = ['SkipMeals', 'WakeTime', 'HoursComputer', 'RaceEthnicity', 'CareerSatisfaction']  # Removed: 'JobSatisfaction' because... that's too easy\n",
    "# features = ['SkipMeals', 'WakeTime', 'CareerSatisfaction']\n",
    "\n",
    "# Filter Features:\n",
    "data = data[features]\n",
    "\n",
    "# We want to predict CareerSatisfaction\n",
    "classification = 'CareerSatisfaction'\n",
    "# We want this classification to be binary. We will range it from not satisfied to satisfied -1 to +1\n",
    "replacement_keys = {'Extremely satisfied': 1, 'Neither satisfied nor dissatisfied': 1,\n",
    "                    'Moderately satisfied': 1, 'Slightly dissatisfied': -1, 'Slightly satisfied': 1,\n",
    "                    'Moderately dissatisfied': -1, 'Extremely dissatisfied': -1}\n",
    "\n",
    "data = data.replace({classification: replacement_keys})  # Target is now binary\n",
    "\n",
    "data = data.dropna(axis=0).reset_index(drop=True)  # Drop Null or nan records.\n",
    "print(f'Rows was {n_rows} before dropping, but now is: {data.shape[0]}')\n",
    "# Convert String columns into one-hot encoded columns\n",
    "for column in data:\n",
    "    if data[column].dtype == object:\n",
    "        # print(f'Encoding one-hot for column: {column} \\r')\n",
    "        data[column] = LabelEncoder().fit_transform(y=data[column].fillna('0'))  # TODO save an array of LabelEncoders\n",
    "\n",
    "# Show correlation Matrix for this data set\n",
    "show_correlation(features, data, n_rows, is_labeled=True)\n",
    "\n",
    "# Split the data into features and targets\n",
    "x = pd.DataFrame.copy(data.drop(classification, axis=1))  # Exclude the classification field from the training samples\n",
    "y = pd.DataFrame.copy(data.drop([f for f in features if f != classification], axis=1))\n",
    "print(\"Split data\")\n",
    "\n",
    "# Split the features into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import *\n",
    "\n",
    "# Super class for machine learning models \n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\" Super class for ITCS Machine Learning Class\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def use(self, X):\n",
    "        pass\n",
    "\n",
    "class Classifier(BaseModel):\n",
    "    \"\"\"\n",
    "        Abstract class for classification \n",
    "        \n",
    "        Attributes\n",
    "        ==========\n",
    "        meanX       ndarray\n",
    "                    mean of inputs (from standardization)\n",
    "        stdX        ndarray\n",
    "                    standard deviation of inputs (standardization)\n",
    "    \"\"\"\n",
    "    def __init__(self, ):\n",
    "        self.meanX = None\n",
    "        self.stdX = None\n",
    "        \n",
    "    def normalize(self, X, reset_fields=False):\n",
    "        \"\"\" standardize the input X \"\"\"\n",
    "\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.asanyarray(X)\n",
    "\n",
    "        if reset_fields:\n",
    "            self.means = np.mean(X, 0)\n",
    "            self.stds = np.std(X, 0)\n",
    "\n",
    "        Xs = (X - self.means) / self.stds\n",
    "        return Xs\n",
    "\n",
    "    def _check_matrix(self, mat, name):\n",
    "        if len(mat.shape) != 2:\n",
    "            raise ValueError(''.join([\"Wrong matrix \", name]))\n",
    "        \n",
    "    # add a basis\n",
    "    def add_ones(self, X):\n",
    "        \"\"\"\n",
    "            add a column basis to X input matrix\n",
    "        \"\"\"\n",
    "        self._check_matrix(X, 'X')\n",
    "        return np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "    ####################################################\n",
    "    #### abstract funcitons ############################\n",
    "    @abc.abstractmethod\n",
    "    def train(self, X, T):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def use(self, X):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Pocket Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PerceptronPocketClassifier(Classifier):\n",
    "\n",
    "    def __init__(self, max_iterations: int, alpha: float = 0.1) -> object:\n",
    "        \"\"\"\n",
    "\n",
    "        :param max_iterations:\n",
    "        :param alpha:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.max_iterations = max_iterations  # training iterations\n",
    "        # Weight matrix\n",
    "        self.w = np.random.uniform(-1.0, 1.0, 1).reshape(-1, 1)\n",
    "        self.w_pocket = np.copy(self.w)\n",
    "\n",
    "    def _compare(self, x, targets):\n",
    "        y = np.sign(x @ self.w)\n",
    "        yp = np.sign(x @ self.w_pocket)\n",
    "\n",
    "        return 1 if np.sum(y == targets) >= np.sum(yp == targets) else -1\n",
    "\n",
    "    def train(self, x: np.ndarray, targets: np.ndarray):\n",
    "        # Set Shape positions\n",
    "        shape_features = x.shape[1]\n",
    "        shape_num_samples = x.shape[0]\n",
    "        shape_target_features = targets.shape[1]\n",
    "\n",
    "        # Reset the w to reflect the dimensions being trained on\n",
    "        self.w = np.zeros(shape_features).reshape(-1, 1)#np.random.uniform(-1.0, 1.0, shape_features).reshape(-1, shape_target_features)\n",
    "        self.w_pocket = np.copy(self.w)\n",
    "\n",
    "        # Normalize Training Data:\n",
    "        x = self.normalize(x, reset_fields=True)\n",
    "\n",
    "        for j in range(self.max_iterations):\n",
    "            print(f'Iteration: {j}')\n",
    "            converged = True\n",
    "            for k in np.random.permutation(shape_num_samples - 1):\n",
    "                '''\n",
    "                ok so, we need weight (W) to be (target_dim X feature_num)\n",
    "                so....\n",
    "                T[k] * X[k] is not enough. Note that this would output (feature X 1)\n",
    "                This is too constraining. This means that each target has to be a scalar,\n",
    "                and X cant have upper dimensionality. This also does not take advantage of matrix \n",
    "                operations.\n",
    "\n",
    "                So we fix this by:\n",
    "\n",
    "                transpose(T dot transpose(X))\n",
    "\n",
    "                because T is (t_features X constant) and X is (features X constant) \n",
    "                and we want (features X constant) outputted\n",
    "\n",
    "                '''\n",
    "                y = np.transpose(self.w) @ x[k]\n",
    "\n",
    "                if np.sign(y) != np.sign(targets[k]):\n",
    "                    self.w += self.alpha * x[k].reshape(-1, 1) * targets[k].reshape(-1, 1)\n",
    "                    converged = False\n",
    "                    if self._compare(x, targets) > 0:\n",
    "                        self.w_pocket[:] = self.w[:]\n",
    "\n",
    "            if converged:\n",
    "                print(\"converged at \", j)\n",
    "                break\n",
    "\n",
    "    def use(self, x: np.ndarray):\n",
    "        x = self.normalize(x)\n",
    "        return x @ self.w_pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!!\n",
    "clf = PerceptronPocketClassifier(1000, 0.1)\n",
    "clf.train(X_train, y_train)\n",
    "\n",
    "print(f'Predicted: \\n\\n {clf.use(X_test)} \\n\\nActual: {y_test}')\n",
    "show_accuracy(clf.use(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "# noinspection PyMethodMayBeStatic\n",
    "class QDAClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.discriminant_functions = {}\n",
    "        self.discriminant_function_params = {}\n",
    "\n",
    "        self.global_mean = 0\n",
    "        self.global_stds = 0\n",
    "\n",
    "    def train(self, big_x: np.array, big_t: np.array):\n",
    "        # Scale the big_x sample base\n",
    "        scaled_big_x = self.normalize(big_x, reset_fields=True)\n",
    "\n",
    "        # Split them by their classes\n",
    "        for unique in [np.unique(ll) for ll in big_t]:\n",
    "            # Get the samples that have that unique value\n",
    "            indexes = np.where(big_t == unique)\n",
    "            temp_x = np.copy(scaled_big_x[indexes])\n",
    "            mu = np.mean(temp_x, 0)\n",
    "            sigma = np.cov(temp_x.T).reshape(-1, 1)\n",
    "            prior = float((len(big_t[big_t == unique[0]]) / len(big_t)))\n",
    "            # Get and save the discriminant function\n",
    "\n",
    "            self.discriminant_function_params[unique[0]] = (mu, sigma, prior)\n",
    "\n",
    "    def get_qda(self, big_x: np.array, mu, sigma, prior):\n",
    "        sigma_inv = np.linalg.inv(sigma)\n",
    "        diff_v = big_x - mu\n",
    "        return - 0.5 * np.log(np.linalg.det(sigma)) \\\n",
    "               - 0.5 * np.sum(diff_v @ sigma_inv * diff_v, axis=1) + np.log(prior)\n",
    "\n",
    "    def use(self, big_x):\n",
    "\n",
    "        scaled_big_x = self.normalize(big_x)\n",
    "        classes = [c for c in self.discriminant_function_params]\n",
    "        evaluations = []\n",
    "        for sample in scaled_big_x:\n",
    "            probabilities: List[float] = []\n",
    "            for class_value in self.discriminant_function_params:\n",
    "                print(f'\\n\\nSample: {sample} Class to test: {class_value} the resulting prob: '\n",
    "                      f'{self.get_qda(np.array(sample).reshape(-1, 1), *self.discriminant_function_params[class_value])}')\n",
    "\n",
    "                probabilities\\\n",
    "                    .append(max(self.get_qda(np.array(sample).reshape(-1, 1),\n",
    "                                             *self.discriminant_function_params[class_value])))\n",
    "            evaluations.append(classes[np.argmax(probabilities)])\n",
    "        return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!!\n",
    "clf = QDAClassifier()\n",
    "clf.train(X_train, y_train)\n",
    "\n",
    "print(f'Predicted: \\n\\n {clf.use(X_test)} \\n\\nActual: {y_test}')\n",
    "show_accuracy(clf.use(X_test), y_test)\n",
    "\n",
    "show_accuracy(clf.use(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# noinspection PyMethodMayBeStatic\n",
    "class LDAClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.discriminant_functions = {}\n",
    "        self.discriminant_function_params = {}\n",
    "\n",
    "        self.global_mean = 0\n",
    "        self.global_stds = 0\n",
    "\n",
    "    def train(self, big_x: np.array, big_t: np.array):\n",
    "        # Scale the big_x sample base\n",
    "        scaled_big_x = self.normalize(big_x, reset_fields=True)\n",
    "\n",
    "        sigma = np.cov(scaled_big_x.T)\n",
    "\n",
    "        # Split them by their classes\n",
    "        for unique in [np.unique(ll) for ll in big_t]:\n",
    "            # Get the samples that have that unique value\n",
    "            indexes = np.where(big_t == unique)\n",
    "            temp_x = np.copy(scaled_big_x[indexes])\n",
    "            mu = np.mean(temp_x, 0)\n",
    "            prior = float((len(big_t[big_t == unique[0]]) / len(big_t)))\n",
    "            # Get and save the discriminant function\n",
    "\n",
    "            self.discriminant_function_params[unique[0]] = (mu, sigma, prior)\n",
    "\n",
    "    def get_lda(self, big_x: np.array, mu, sigma, prior):\n",
    "        sigma_inv = np.linalg.inv(sigma)\n",
    "        return np.sum(np.dot(big_x, sigma_inv) * mu -\n",
    "                      0.5 * np.dot(mu, sigma_inv) * mu\n",
    "                      + np.log(prior), axis=1)\n",
    "\n",
    "    def use(self, big_x):\n",
    "\n",
    "        scaled_big_x = self.normalize(big_x)\n",
    "        classes = [c for c in self.discriminant_function_params]\n",
    "        evaluations = []\n",
    "        for sample in scaled_big_x:\n",
    "            probabilities: List[float] = []\n",
    "            for class_value in self.discriminant_function_params:\n",
    "                print(f'\\n\\nSample: {sample} Class to test: {class_value} the resulting prob: '\n",
    "                      f'{self.get_lda(np.array(sample).reshape(-1, 1).T, *self.discriminant_function_params[class_value])}')\n",
    "\n",
    "                probabilities\\\n",
    "                    .append(max(self.get_lda(np.array(sample).reshape(-1, 1).T,\n",
    "                                             *self.discriminant_function_params[class_value])))\n",
    "            evaluations.append(classes[np.argmax(probabilities)])\n",
    "        return evaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!!\n",
    "clf = LDAClassifier()\n",
    "clf.train(X_train, y_train)\n",
    "\n",
    "print(f'Predicted: \\n\\n {clf.use(X_test)} \\n\\nActual: {y_test}')\n",
    "show_accuracy(clf.use(X_test), y_test)\n",
    "\n",
    "show_accuracy(clf.use(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# noinspection PyMethodMayBeStatic\n",
    "class LRClassifier(Classifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w = None\n",
    "        self.binarizer = LabelBinarizer()\n",
    "\n",
    "    def _soft_max(self, z: np.ndarray):\n",
    "        if not isinstance(z, np.ndarray):\n",
    "            z = np.asarray(z)\n",
    "        f = np.exp(z)\n",
    "        return f / (np.sum(f, axis=1, keepdims=True)) if len(z.shape) == 2 else np.sum(f)\n",
    "\n",
    "    def _g(self, big_x):\n",
    "        return self._soft_max(big_x @ self.w)\n",
    "\n",
    "    def train(self, big_x: np.ndarray, big_t: np.ndarray, iterations=1000, alpha=0.1):\n",
    "        big_x = self.normalize(big_x, reset_fields=True)\n",
    "\n",
    "        # Fix big_t to be one hot (one column for each class)\n",
    "        one_hot_big_t = self.one_hot(big_t)\n",
    "\n",
    "        n_samples = big_x.shape[0]\n",
    "        n_features = big_x.shape[1]\n",
    "        n_target_dims = one_hot_big_t.shape[1]\n",
    "        n_bias_dims = 1\n",
    "        self.w = np.random.rand(n_features + n_bias_dims, n_target_dims)\n",
    "\n",
    "        # Add a bias column to big_x\n",
    "        bias_big_x = np.hstack((np.ones((n_samples, n_bias_dims)), big_x))\n",
    "\n",
    "        for step in range(iterations):\n",
    "            y_scaled = self._g(bias_big_x)\n",
    "            self.w += alpha * bias_big_x.T @ (one_hot_big_t - y_scaled)\n",
    "\n",
    "    def one_hot(self, big_t: np.ndarray):\n",
    "        # One hot via numpy:\n",
    "        self.binarizer.fit(big_t)\n",
    "        labels = self.binarizer.transform(big_t)\n",
    "        return np.hstack((labels, 1 - labels))\n",
    "\n",
    "    def use(self, big_x: np.ndarray):\n",
    "        big_x = self.normalize(big_x)\n",
    "        n_samples = big_x.shape[0]\n",
    "        n_bias_dims = 1\n",
    "        bias_big_x = np.hstack((np.ones((n_samples, n_bias_dims)), big_x))\n",
    "\n",
    "        y = self._g(bias_big_x)\n",
    "\n",
    "        return self.binarizer.inverse_transform(np.argmax(y, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training!!\n",
    "clf = LRClassifier()\n",
    "clf.train(X_train, y_train)\n",
    "\n",
    "print(f'Predicted: \\n\\n {clf.use(X_test)} \\n\\nActual: {y_test}')\n",
    "show_accuracy(clf.use(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Experiments\n",
    "\n",
    "Apply the classfiers on the data and discuss the results.\n",
    "Please describe your codes for experiments. You may have subsections of results and discussions here.\n",
    "Here follows the list that you consider to include:\n",
    "- the classification results\n",
    "- plots of classification results \n",
    "- model comparision \n",
    "- choice of evaluation metrics\n",
    "- **Must partition data into training and testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Summarize your work here. \n",
    "Which classifier do you think the best? \n",
    "Discuss the challenges or somethat that you learned. \n",
    "If you have any suggestion about the assignment, you can write about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "List all your references here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit\n",
    "\n",
    "* [OPT 1] Search for a ordinal data set and apply your classifiers to it. \n",
    "  - Repeat the experiments on it. \n",
    "  - Do you have different observation from previous results? \n",
    "  - Were you able to observe that we discussed in class about logistic regression? \n",
    "  - For a full extra credit point, you need to discuss all bullet points in Results section.     \n",
    "\n",
    "\n",
    "* [OPT 2] Partition your data into five sets. Selecting one test set and the other for training, repeat your experiments and observe/analyze the 5 different training/testing errors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "DO NOT forget to submit your data! Your notebook is supposed to run well after running your codes.\n",
    "\n",
    "To help our TA's grading, please make an explicit section for each grading criteria. \n",
    "Again, this is a **writing assignment**. Please don't forget to properly explain your codes and results using Markdown cell. \n",
    "\n",
    "\n",
    "points | | description\n",
    "--|--|:--\n",
    "5 | Overview| states the objective and the appraoch \n",
    "15 | Data | \n",
    " | 5| description \n",
    " | 5| plots for understanding or analysis \n",
    " | 5| preliminary observation \n",
    "25 | Methods | \n",
    " |10| Summary of Classification models\n",
    " | 5| Explanation of codes\n",
    " |10| Pocket, LDA, QDA, Logistic Regression\n",
    "40 | Experiments \n",
    "| 5| Discussion about evaluation metrics\n",
    "| 5| Discussion about train and test accuracies\n",
    "|20| plots for results (5 for each algorithm)\n",
    "|10| Discussions about classificaion model comparison\n",
    "5 | |Conclusions \n",
    "5 | |Referemces\n",
    "5 | |Grammar and spelling error (Proofread please)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
